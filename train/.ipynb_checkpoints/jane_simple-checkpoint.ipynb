{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start by fetching and preprocessing our text\n",
    "import re\n",
    "import os\n",
    "\n",
    "def clean_text(text):\n",
    "    with open(book, 'r', encoding='utf8', errors='ignore') as f:\n",
    "        #transform everything to lowercase\n",
    "        text = f.read().lower()\n",
    "    \n",
    "        #replace markdown characters\n",
    "        text = text.replace('\\xa0', ' ')\n",
    "    \n",
    "        #remove emoji and author name\n",
    "        text = re.sub(r\"✍ jane austen\", \"\", text)\n",
    "        \n",
    "        #remove these chars: they're not counted as punctuation later when they're attached to a word with no space\n",
    "        #which is something that happens a lot in Austen's dialogues\n",
    "        text = text.replace('—', ' ')\n",
    "        text = text.replace('_', ' ')\n",
    "    \n",
    "        #remove numbers and stuff like 1st, 2nd, 3rd, 4th, etc.\n",
    "        text = re.sub(r\"\\d+\\w*\", \" \", text)\n",
    "   \n",
    "        #remove volume and chapter headings\n",
    "        text = re.sub(r\"#+\\s\\w+\\s\\w+\\s\", \"\", text)\n",
    "        \n",
    "        #remove book title\n",
    "        text = re.sub(r\"#\\s\\w+\\s\", \"\", text)\n",
    "    \n",
    "        #remove weird chars\n",
    "        text = text.replace(\"£\", \" \")\n",
    "        text = re.sub(r\"\\n\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "#loop through all the books we've saved as text files in our data dir and create a final string of cleaned text\n",
    "cleaned_text = ''\n",
    "for file in os.listdir(\"data\"):\n",
    "    book = \"data/\" + file\n",
    "    cleaned_text += ' ' + clean_text(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will transform our text into a format that can be understood by NLP algorithms\n",
    "import nltk\n",
    "#import string\n",
    "\n",
    "#tokenize cleaned_text string to a list of sentences\n",
    "sent_tokens = nltk.sent_tokenize(cleaned_text)\n",
    "\n",
    "#extract dialog from complete text sentences\n",
    "def extract_dialog(text):\n",
    "    dialog_pattern = '(?<=“)(.+?)(?=”)'\n",
    "    \n",
    "    m = re.search(dialog_pattern, text)\n",
    "    \n",
    "    if m is not None:\n",
    "        return m.group(0)\n",
    "\n",
    "dialog = [extract_dialog(sentence) for sentence in sent_tokens if extract_dialog(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a matrix of TFIDF weights for our sentences\n",
    "#we will need this to compare to new text (i.e. the user request) with a similarity measure\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "TfidfVec = TfidfVectorizer(stop_words='english')\n",
    "tfidf = TfidfVec.fit_transform(dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   abbey  abhor  abide  able  abode  abominable  abominably  abroad  absence  \\\n",
      "0    0.0    0.0    0.0   0.0    0.0         0.0         0.0     0.0      0.0   \n",
      "1    0.0    0.0    0.0   0.0    0.0         0.0         0.0     0.0      0.0   \n",
      "2    0.0    0.0    0.0   0.0    0.0         0.0         0.0     0.0      0.0   \n",
      "3    0.0    0.0    0.0   0.0    0.0         0.0         0.0     0.0      0.0   \n",
      "4    0.0    0.0    0.0   0.0    0.0         0.0         0.0     0.0      0.0   \n",
      "\n",
      "   absent  ...   ye  year  years  yes  yesterday  yield  york  young  younger  \\\n",
      "0     0.0  ...  0.0   0.0    0.0  0.0        0.0    0.0   0.0    0.0      0.0   \n",
      "1     0.0  ...  0.0   0.0    0.0  0.0        0.0    0.0   0.0    0.0      0.0   \n",
      "2     0.0  ...  0.0   0.0    0.0  0.0        0.0    0.0   0.0    0.0      0.0   \n",
      "3     0.0  ...  0.0   0.0    0.0  0.0        0.0    0.0   0.0    0.0      0.0   \n",
      "4     0.0  ...  0.0   0.0    0.0  0.0        0.0    0.0   0.0    0.0      0.0   \n",
      "\n",
      "   youngest  \n",
      "0       0.0  \n",
      "1       0.0  \n",
      "2       0.0  \n",
      "3       0.0  \n",
      "4       0.0  \n",
      "\n",
      "[5 rows x 2550 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#the result of this is a matrix, X, with one row for each sentence (\"document\") and a column for each unique word\n",
    "X = pd.DataFrame(tfidf.toarray(), columns = TfidfVec.get_feature_names(), dtype='float32')\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a corpus represented in such a way that we can use it to, for example, calculate distances to other (similarly transformed) text. \n",
    "\n",
    "I'm using cosine similarity: how \"close\" is our query (user input) to our tokenized dialog?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in-place chatbot feature will let us talk with the bot here locally with no server setup\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def response(user_response):\n",
    "    jane_response = ''\n",
    "\n",
    "    #transform user query so we can compare it to our matrix of TFIDF weighted word features\n",
    "    query = TfidfVec.transform([user_response])\n",
    "    \n",
    "    #what's the maximum closeness we can achieve to our saved dialogues?\n",
    "    cosine_sim = query.dot(X.T)\n",
    "    \n",
    "    #if there's nothing like the user query in our matrix, give a standard response\n",
    "    if cosine_sim.argmax() == 0:\n",
    "        return 'I beg your pardon? I\\'m not quite sure I got your meaning.'\n",
    "    \n",
    "    #otherwise, return the closest dialog to the user request\n",
    "    jane_response = dialog[cosine_sim.argmax()]  \n",
    "    \n",
    "    #take the puncutation off of the end of our response: this is usually a comma in Austen's dialogues!\n",
    "    jane_response = jane_response[0:-1] + '.'\n",
    "    \n",
    "    #capitalize the first letter of the response so that it looks like a real sentence\n",
    "    #remember that the first step we took in cleaning the text was to transform everything to lowercase\n",
    "    return jane_response.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JANE: My name is Miss Austen. I will answer any questions you have about my world! If you would like to end the conversation prematurely, please type 'Thank you'\n",
      "Good Morning\n",
      "JANE: Good morning to you.\n",
      "How are you today\n",
      "JANE: No walk for me today.\n",
      "OK. Thank you\n",
      "JANE: No, no not at all no, thank yo.\n",
      "Thank YOU\n",
      "JANE: You are most welcome. Goodbye now.\n"
     ]
    }
   ],
   "source": [
    "#prompt a dialog with the user\n",
    "print(\"JANE: My name is Miss Austen. I will answer any questions you have about my world! If you would like to end the conversation prematurely, please type \\'Thank you\\'\")\n",
    "\n",
    "#while the dialog is ongoing...\n",
    "flag = True\n",
    "while(flag == True):\n",
    "    #transform user input and fetch a response from our matrix\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    \n",
    "    #the cue to end the conversation\n",
    "    if(user_response == 'thank you'):\n",
    "        flag=False\n",
    "        print(\"JANE: You are most welcome. Goodbye now.\")\n",
    "    else:\n",
    "        print(\"JANE: \", end=\"\")\n",
    "        print(response(user_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#serialize the components of our model so we can upload them to AWS S3 reuse them in our AWS Lambda architecture:\n",
    "#we don't want to have to retrain our model and recreate the TFIDF matrix every time the user sends a query over Slack\n",
    "import pickle\n",
    "    \n",
    "pickle.dump(X, open('model_simple_X.pkl', 'wb'))\n",
    "pickle.dump(dialog, open('model_simple_dialog.pkl', 'wb'))\n",
    "pickle.dump(TfidfVec, open('model_simple_tfidf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good morning to you,\n"
     ]
    }
   ],
   "source": [
    "#did it work?\n",
    "# Load from file\n",
    "pickle_X = pickle.load(open('model_simple_X.pkl', 'rb'))\n",
    "pickle_dialog = pickle.load(open('model_simple_dialog.pkl', 'rb'))\n",
    "pickle_tfidf = pickle.load(open('model_simple_tfidf.pkl', 'rb'))\n",
    "\n",
    "query = pickle_tfidf.transform(['Good morning'])\n",
    "cosine_sim = query.dot(pickle_X.T)\n",
    "reply = pickle_dialog[cosine_sim.argmax()]\n",
    "\n",
    "print(reply)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
